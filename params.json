{"name":"wpget","body":"### Welcome to GitHub Pages.\r\n**wpget** is a tiny Python script that (recursively) downloads a set of pages from Wikipedia into a local sqlite database. Its purpose is to quickly create a local subset of WP articles for further data mining/processing.\r\n\r\n```\r\n$ wpget --recursive Category:Media_companies_of_Germany\r\n$ wpget --lang=de \"Axel Springer AG\"\r\n```\r\n\r\n### Usage\r\nHere's the full usage help text:\r\n\r\n```\r\nusage: wpget [-h] [--recursive] [--lang LANG] [--version] [--database FILE]\r\n             [--content-dir PATH]\r\n             TARGET\r\n\r\nretreives a reasonable set of wikipedia pages. limited to 500 pages per\r\ncategory.\r\n\r\npositional arguments:\r\n  TARGET                name of page or category, eg\"Barack Obama\"\r\n                        or\"Category:Presidents_of_the_United_States\"\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --recursive, -r       if a category url is given, wpget will also process\r\n                        sub-categories recursivly\r\n  --lang LANG, -l LANG  retreive pages from different locales than\"en\"\r\n  --version             show program's version number and exit\r\n  --database FILE, -d FILE\r\n                        specifies which database wpget should write to\r\n                        (defaults to wpget.db)\r\n  --content-dir PATH, -c PATH\r\n                        specifies the path where wpget should store page\r\n                        contents (one file per page). otherwise content will\r\n                        be stored in sqlite database\r\n```\r\n\r\n### Authors and Contributors\r\nwpget was written by Gregor Aisch (@gka).\r\n\r\n### Support or Contact\r\nPlease submit [new issues at Github](https://github.com/gka/wpget/issues).","tagline":"Downloads a set of pages from Wikipedia into a local sqlite database.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}